{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import combinations\n",
    "import matplotlib\n",
    "import csv\n",
    "import copy\n",
    "from shakespear_extra import SHAKESPEAR_GENRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAKEDRACOR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function if network is split into disconnected parts\n",
    "def get_largest_G(input_G):\n",
    "    if nx.is_connected(input_G) is False:\n",
    "        components = list(nx.connected_components(input_G))\n",
    "        small_components = [f for f in components if len(f) != max([len(c) for c in components])]\n",
    "        G_copy = input_G.copy()\n",
    "        for component in small_components:\n",
    "            for node in component:\n",
    "                G_copy.remove_node(node)\n",
    "        return G_copy\n",
    "    else:\n",
    "        return input_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open xml files and parse soups\n",
    "shake_soups = {}\n",
    "for xml_path in Path('shakedracor/tei').glob('*.xml'):\n",
    "    with open(xml_path, 'r') as fh:\n",
    "        shake_soups[xml_path.stem] = BeautifulSoup(fh.read(), 'lxml-xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get significant characters who have their own dialouge - THIS IS FOR FURTHER ANALYSIS, NOT FOR MATCHING WHOLE DRACORE NETWORK\n",
    "significant_characters = []\n",
    "for sp in shake_soups['coriolanus'].find_all('sp', {'who':True}):\n",
    "    speakers = sp['who'].split(' ')\n",
    "    if len(speakers) == 1 and speakers[0] not in significant_characters:\n",
    "        significant_characters.extend(speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate div type=\"act\" tags and extract all characters, create combinations of 2 and add combination to list of edges\n",
    "G_list = {}\n",
    "for name, soup in shake_soups.items():\n",
    "    edge_list = []\n",
    "    lonely_nodes = []\n",
    "    significant_characters = []\n",
    "    for scene in soup.find_all('div', {'type': 'scene'}):\n",
    "        scene_characters = []\n",
    "        for sp in scene.find_all('sp', {'who': True}):\n",
    "            speakers = sp['who'].split(' ')\n",
    "\n",
    "            # UNCOMMENT FOR FURTHER ANALYSIS USING ONLY SIGNIFICANT CHARACTERS\n",
    "#             if len(speakers) == 1 and speakers[0] not in significant_characters:\n",
    "#                 significant_characters.extend(speakers)\n",
    "\n",
    "            for split_sp in speakers:\n",
    "                scene_characters.append(split_sp)\n",
    "            \n",
    "        if len(scene_characters) == 1:\n",
    "            lonely_nodes.append(scene_characters[0])\n",
    "            \n",
    "        scene_edge_list = list(combinations(set(scene_characters), 2))\n",
    "        edge_list += scene_edge_list\n",
    "    \n",
    "    whole = nx.from_edgelist(set(edge_list))\n",
    "    \n",
    "    for lonely_node in lonely_nodes:\n",
    "        if lonely_node not in whole.nodes:\n",
    "            whole.add_node(lonely_node)\n",
    "    \n",
    "    # UNCOMMENT FOR FURTHER ANALYSIS USING ONLY SIGNIFICANT CHARACTERS\n",
    "    # remove those who have no independent dialouge\n",
    "#     removed_nodes = []\n",
    "#     for node in list(whole.nodes()):\n",
    "#         if node not in significant_characters:\n",
    "#             whole.remove_node(node)\n",
    "#             removed_nodes.append(node)\n",
    "    \n",
    "                \n",
    "    G_list[name] = {'whole': whole, \n",
    "                    'title_pretty': soup.find('title').get_text(strip=True), \n",
    "                    'kept_characters': ', '.join(list(whole.nodes())),\n",
    "                    'character_count': len(whole.nodes()),\n",
    "#                     'removed_characters': ', '.join(removed_nodes),\n",
    "#                     'removed_characters_count': len(removed_nodes)\n",
    "                   }\n",
    "    \n",
    "\n",
    "for name, soup in shake_soups.items():\n",
    "    for n in [1,2,3,4,5]:\n",
    "        lonely_nodes = []\n",
    "        edge_list = []\n",
    "        significant_characters = []\n",
    "    \n",
    "        for act in soup.find_all('div', {'type': 'act'}):\n",
    "            if act['n'] != str(n):\n",
    "                for scene in act.find_all('div', {'type': 'scene'}):\n",
    "                    scene_characters = []\n",
    "                    for sp in scene.find_all('sp', {'who': True}):\n",
    "                        speakers = sp['who'].split(' ')\n",
    "                        if len(speakers) == 1 and speakers[0] not in significant_characters:\n",
    "                            significant_characters.extend(speakers)\n",
    "                        for split_sp in speakers:\n",
    "                            scene_characters.append(split_sp)\n",
    "                            scene_edge_list = list(combinations(set(scene_characters), 2))\n",
    "                    if len(scene_characters) == 1:\n",
    "                        lonely_nodes.append(scene_characters[0])\n",
    "                    edge_list += scene_edge_list\n",
    "        \n",
    "        # create network from edge list\n",
    "        whole = nx.from_edgelist(set(edge_list))\n",
    "    \n",
    "        # add those with independent scenes\n",
    "        for lonely_node in lonely_nodes:\n",
    "            if lonely_node not in whole.nodes:\n",
    "                whole.add_node(lonely_node)\n",
    "    \n",
    "        # UNCOMMENT FOR FURTHER ANALYSIS USING ONLY SIGNIFICANT CHARACTERS\n",
    "        # remove those who have no independent dialouge\n",
    "#         for node in list(whole.nodes()):\n",
    "#             if node not in significant_characters:\n",
    "#                 whole.remove_node(node)\n",
    "                \n",
    "        G_list[name]['wo'+str(n)] = whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**CSV WRITING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_names = ['title', 'title_pretty', 'genre', 'kept_characters', 'character_count', 'removed_characters', 'removed_characters_count']\n",
    "metric_names = ['density', 'diameter', 'average_clustering']\n",
    "for w in ['whole', 'wo1', 'wo2', 'wo3', 'wo4', 'wo5']:\n",
    "    for n in metric_names:\n",
    "        column_names.append(w+'_'+n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('shakedracore_metrics8_misitest.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = column_names\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for name, graph_dict in G_list.items():\n",
    "        drama_dict = {'title': name,\n",
    "                      'title_pretty': graph_dict['title_pretty'],\n",
    "                      'genre': SHAKESPEAR_GENRES[name],\n",
    "                      'kept_characters': graph_dict['kept_characters'],\n",
    "                      'character_count': graph_dict['character_count'],\n",
    "#                       'removed_characters': graph_dict['removed_characters'],\n",
    "#                       'removed_characters_count': graph_dict['removed_characters_count']\n",
    "                     }\n",
    "        \n",
    "        drama_dict['whole_density'] = nx.density(graph_dict['whole'])\n",
    "        drama_dict['whole_diameter'] = nx.diameter(get_largest_G(graph_dict['whole']))\n",
    "        drama_dict['whole_average_clustering'] = nx.average_clustering(graph_dict['whole'])\n",
    "        \n",
    "        for w in ['wo1', 'wo2', 'wo3', 'wo4', 'wo5']:\n",
    "            drama_dict[w+'_density'] = nx.density(graph_dict[w])\n",
    "            drama_dict[w+'_diameter'] = nx.diameter(get_largest_G(graph_dict[w]))\n",
    "            drama_dict[w+'_average_clustering'] = nx.average_clustering(graph_dict[w])\n",
    "        \n",
    "        \n",
    "        writer.writerow(drama_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "for name, G in G_list.items():\n",
    "    print(f'{name:>15} DENSITY:', nx.density(G))\n",
    "    print(f'{name:>15} DIAMETER:', nx.diameter(G_list_connected[name]))\n",
    "    print(f'{name:>15} AVERAGE CLUSTERING:', nx.average_clustering(G), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
