{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE SURE YOU HAVE NECESSARY REQUIREMENTS AND CORPUSES AS TEI FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from shakespear_extra import SHAKESPEAR_GENRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespear_tei_files = 'shakedracor/tei'\n",
    "gerdracor_tei_files = 'gerdracor/tei'\n",
    "original_gerdracor_metadata_file = 'gerdracor-metadata.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HELPERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_G(input_G, name=None):\n",
    "    \"\"\"\n",
    "    Function to extract largest connected section of nx Graph object.\n",
    "    \"\"\"\n",
    "    if len(input_G.nodes) == 0:\n",
    "        raise ValueError(f'ZERO NODE GRAPH PASSED TO get_largest_G - {input_G}: {name}')\n",
    "    if nx.is_connected(input_G) is False:\n",
    "        # larges connected section\n",
    "        nodes_in_largest = max(nx.connected_components(input_G), key=len)\n",
    "        nodes_to_remove = set(input_G.nodes) - nodes_in_largest\n",
    "        G_copy = input_G.copy()\n",
    "        G_copy.remove_nodes_from(nodes_to_remove)\n",
    "        return G_copy\n",
    "    else:\n",
    "        return input_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>SHAKESPEAR ANALYSIS</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open xml files and parse soups\n",
    "shake_soups = {}\n",
    "for xml_path in Path(shakespear_tei_files).glob('*.xml'):\n",
    "    with open(xml_path, 'r') as fh:\n",
    "        shake_soups[xml_path.stem] = BeautifulSoup(fh.read(), 'lxml-xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get significant characters who have their own dialouge - THIS IS FOR FURTHER ANALYSIS, NOT FOR MATCHING WHOLE DRACORE NETWORK\n",
    "significant_characters = []\n",
    "for sp in shake_soups['coriolanus'].find_all('sp', {'who':True}):\n",
    "    speakers = sp['who'].split(' ')\n",
    "    if len(speakers) == 1 and speakers[0] not in significant_characters:\n",
    "        significant_characters.extend(speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate div type=\"act\" tags and extract all characters, create combinations of 2 and add combination to list of edges\n",
    "G_list = {}\n",
    "for name, soup in shake_soups.items():\n",
    "    edge_list = []\n",
    "    lonely_nodes = []\n",
    "    significant_characters = []\n",
    "    for scene in soup.find_all('div', {'type': 'scene'}):\n",
    "        scene_characters = []\n",
    "        for sp in scene.find_all('sp', {'who': True}):\n",
    "            speakers = sp['who'].split(' ')\n",
    "\n",
    "            for split_sp in speakers:\n",
    "                scene_characters.append(split_sp)\n",
    "            \n",
    "        if len(scene_characters) == 1:\n",
    "            lonely_nodes.append(scene_characters[0])\n",
    "            \n",
    "        scene_edge_list = list(combinations(set(scene_characters), 2))\n",
    "        edge_list += scene_edge_list\n",
    "    \n",
    "    whole = nx.from_edgelist(set(edge_list))\n",
    "    \n",
    "    for lonely_node in lonely_nodes:\n",
    "        if lonely_node not in whole.nodes:\n",
    "            whole.add_node(lonely_node)\n",
    "    \n",
    "    G_list[name] = {'whole': whole, \n",
    "                    'title_pretty': soup.find('title').get_text(strip=True), \n",
    "                    'kept_characters': ', '.join(list(whole.nodes())),\n",
    "                    'character_count': len(whole.nodes()),\n",
    "                   }\n",
    "    \n",
    "\n",
    "for name, soup in shake_soups.items():\n",
    "    for n in [1,2,3,4,5]:\n",
    "        lonely_nodes = []\n",
    "        edge_list = []\n",
    "    \n",
    "        for act in soup.find_all('div', {'type': 'act'}):\n",
    "            if act['n'] != str(n):\n",
    "                for scene in act.find_all('div', {'type': 'scene'}):\n",
    "                    scene_characters = []\n",
    "                    for sp in scene.find_all('sp', {'who': True}):\n",
    "                        speakers = sp['who'].split(' ')\n",
    "                        \n",
    "                        for split_sp in speakers:\n",
    "                            scene_characters.append(split_sp)\n",
    "                        scene_edge_list = list(combinations(set(scene_characters), 2))\n",
    "                    if len(scene_characters) == 1:\n",
    "                        lonely_nodes.append(scene_characters[0])\n",
    "                    edge_list += scene_edge_list\n",
    "        \n",
    "        # create network from edge list\n",
    "        whole = nx.from_edgelist(set(edge_list))\n",
    "    \n",
    "        # add those with independent scenes\n",
    "        for lonely_node in lonely_nodes:\n",
    "            if lonely_node not in whole.nodes:\n",
    "                whole.add_node(lonely_node)\n",
    "    \n",
    "        G_list[name]['wo'+str(n)] = whole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**CSV WRITING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "csv_filename = 'shakedracore_metrics8.csv'\n",
    "column_names = ['title', 'title_pretty', 'genre', 'kept_characters', 'character_count', 'removed_characters', 'removed_characters_count']\n",
    "metric_names = ['density', 'diameter', 'average_clustering']\n",
    "for w in ['whole', 'wo1', 'wo2', 'wo3', 'wo4', 'wo5']:\n",
    "    for n in metric_names:\n",
    "        column_names.append(w+'_'+n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    fieldnames = column_names\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for name, graph_dict in G_list.items():\n",
    "        drama_dict = {'title': name,\n",
    "                      'title_pretty': graph_dict['title_pretty'],\n",
    "                      'genre': SHAKESPEAR_GENRES[name],\n",
    "                      'kept_characters': graph_dict['kept_characters'],\n",
    "                      'character_count': graph_dict['character_count'],\n",
    "#                       'removed_characters': graph_dict['removed_characters'],\n",
    "#                       'removed_characters_count': graph_dict['removed_characters_count']\n",
    "                     }\n",
    "        \n",
    "        drama_dict['whole_density'] = nx.density(graph_dict['whole'])\n",
    "        drama_dict['whole_diameter'] = nx.diameter(get_largest_G(graph_dict['whole']))\n",
    "        drama_dict['whole_average_clustering'] = nx.average_clustering(graph_dict['whole'])\n",
    "        \n",
    "        for w in ['wo1', 'wo2', 'wo3', 'wo4', 'wo5']:\n",
    "            drama_dict[w+'_density'] = nx.density(graph_dict[w])\n",
    "            drama_dict[w+'_diameter'] = nx.diameter(get_largest_G(graph_dict[w]))\n",
    "            drama_dict[w+'_average_clustering'] = nx.average_clustering(graph_dict[w])\n",
    "        \n",
    "        \n",
    "        writer.writerow(drama_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUMULATIVE CALC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysising coriolanus acts1 2 3 4 5 Analysising twelfth-night acts1 2 3 4 5 Analysising richard-ii acts1 2 3 4 5 Analysising as-you-like-it acts1 2 3 4 5 Analysising the-tempest acts1 2 3 4 5 Analysising henry-iv-part-ii acts1 2 3 4 5 Analysising the-taming-of-the-shrew acts1 2 3 4 5 Analysising henry-viii acts1 2 3 4 5 Analysising all-s-well-that-ends-well acts1 2 3 4 5 Analysising titus-andronicus acts1 2 3 4 5 Analysising henry-vi-part-1 acts1 2 3 4 5 Analysising henry-vi-part-3 acts1 2 3 4 5 Analysising antony-and-cleopatra acts1 2 3 4 5 Analysising measure-for-measure acts1 2 3 4 5 Analysising julius-caesar acts1 2 3 4 5 Analysising love-s-labor-s-lost acts1 2 3 4 5 Analysising henry-iv-part-i acts1 2 3 4 5 Analysising two-gentlemen-of-verona acts1 2 3 4 5 Analysising romeo-and-juliet acts1 2 3 4 5 Analysising timon-of-athens acts1 2 3 4 5 Analysising a-midsummer-night-s-dream acts1 2 3 4 5 Analysising cymbeline acts1 2 3 4 5 Analysising troilus-and-cressida acts1 2 3 4 5 Analysising king-john acts1 2 3 4 5 Analysising the-comedy-of-errors acts1 2 3 4 5 Analysising henry-vi-part-2 acts1 2 3 4 5 Analysising macbeth acts1 2 3 4 5 Analysising the-merry-wives-of-windsor acts1 2 3 4 5 Analysising king-lear acts1 2 3 4 5 Analysising henry-v acts1 2 3 4 5 Analysising hamlet acts1 2 3 4 5 Analysising the-winter-s-tale acts1 2 3 4 5 Analysising pericles acts1 2 3 4 5 Analysising the-merchant-of-venice acts1 2 3 4 5 Analysising much-ado-about-nothing acts1 2 3 4 5 Analysising othello acts1 2 3 4 5 Analysising richard-iii acts1 2 3 4 5 "
     ]
    }
   ],
   "source": [
    "cumulative_G_list = defaultdict(dict)\n",
    "\n",
    "for name, soup in shake_soups.items():\n",
    "    acts_included = []\n",
    "    print(f'Analysising {name} acts', end='')\n",
    "    for n in ['1','2','3','4','5']:\n",
    "        acts_included.append(n)\n",
    "        print(f'{acts_included[-1]} ', end='')\n",
    "        lonely_nodes = []\n",
    "        edge_list = []\n",
    "        significant_characters = []\n",
    "    \n",
    "        for act in soup.find_all('div', {'type': 'act'}):\n",
    "            # div type act has an attribute 'n' with value of act number.\n",
    "            if act['n'] in acts_included:\n",
    "                for scene in act.find_all('div', {'type': 'scene'}):\n",
    "                    scene_characters = []\n",
    "                    for sp in scene.find_all('sp', {'who': True}):\n",
    "                        speakers = sp['who'].split(' ')\n",
    "                        if len(speakers) == 1 and speakers[0] not in significant_characters:\n",
    "                            significant_characters.extend(speakers)\n",
    "                        for split_sp in speakers:\n",
    "                            scene_characters.append(split_sp)\n",
    "                        scene_edge_list = list(combinations(set(scene_characters), 2))\n",
    "                    if len(scene_characters) == 1:\n",
    "                        lonely_nodes.append(scene_characters[0])\n",
    "                    edge_list += scene_edge_list\n",
    "        \n",
    "        # create network from edge list\n",
    "        n_acts_whole = nx.from_edgelist(set(edge_list))\n",
    "    \n",
    "        # add those with independent scenes\n",
    "        for lonely_node in lonely_nodes:\n",
    "            if lonely_node not in n_acts_whole.nodes:\n",
    "                n_acts_whole.add_node(lonely_node)\n",
    "        \n",
    "        cumulative_G_list[name]['title_pretty'] = soup.find('title').get_text(strip=True)\n",
    "        cumulative_G_list[name][f\"acts_{'-'.join(acts_included)}\"] = n_acts_whole\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSV WRITING CUMULATIVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = 'shakedracor_cumulative2.csv'\n",
    "column_names = ['title', 'title_pretty', 'genre']\n",
    "metric_names = ['density', 'diameter', 'average_clustering']\n",
    "for w in ['acts_1', 'acts_1-2', 'acts_1-2-3', 'acts_1-2-3-4', 'acts_1-2-3-4-5']:\n",
    "    for n in metric_names:\n",
    "        column_names.append(w+'_'+n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=column_names)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for name, drama_data in cumulative_G_list.items():\n",
    "        drama_dict = {'title': name,\n",
    "              'title_pretty': drama_data['title_pretty'],\n",
    "              'genre': SHAKESPEAR_GENRES[name],\n",
    "             }\n",
    "        for w in ['acts_1', 'acts_1-2', 'acts_1-2-3', 'acts_1-2-3-4', 'acts_1-2-3-4-5']:\n",
    "            drama_dict[f'{w}_density'] = nx.density(drama_data[w])\n",
    "            drama_dict[f'{w}_diameter'] = nx.diameter(get_largest_G(drama_data[w]))\n",
    "            drama_dict[f'{w}_average_clustering'] = nx.average_clustering(drama_data[w])\n",
    "        \n",
    "        writer.writerow(drama_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "for name, G in G_list.items():\n",
    "    print(f'{name:>15} DENSITY:', nx.density(G))\n",
    "    print(f'{name:>15} DIAMETER:', nx.diameter(G_list_connected[name]))\n",
    "    print(f'{name:>15} AVERAGE CLUSTERING:', nx.average_clustering(G), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>GERMAN DRAMA ANALYSIS</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GERDRACORE CUMULATIVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open xml files and parse soups\n",
    "ger_soups = {}\n",
    "for xml_path in Path(gerdracor_tei_files).glob('*.xml'):\n",
    "    with open(xml_path, 'r') as fh:\n",
    "        # Filter 5 act, 5+ actor, Tragedy and Comedy dramas from GerDracor\n",
    "        soup = BeautifulSoup(fh.read(), 'lxml-xml')\n",
    "        \n",
    "        acts = soup.find_all('div', {'type': 'act'})\n",
    "        cast_list = soup.find('profileDesc').find('listPerson').find_all('person')\n",
    "        genre_tag = soup.find('textClass').find('keywords').find('term', {'type': 'genreTitle'})\n",
    "        genre = genre_tag.get_text(strip=True)\n",
    "        if cast_list is None:\n",
    "            raise ValueError('No cast list found!')\n",
    "        if len(acts) == 5 and len(cast_list) > 5 and genre in ['Tragedy', 'Comedy']:\n",
    "            ger_soups[xml_path.stem] = soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_appearance_unit_edge_list(unit, lonely_nodes):\n",
    "    \"\"\"\n",
    "    Take unit of BS4 tag and extract all connections between speakers in unit.\n",
    "    \"\"\"\n",
    "    characters = []\n",
    "                \n",
    "    for sp in unit.find_all('sp', {'who': True}):\n",
    "        # get each speaker from speaker tag\n",
    "        speakers = sp['who'].split(' ')\n",
    "\n",
    "        for split_sp in speakers:\n",
    "            characters.append(split_sp)\n",
    "    edge_list = list(combinations(set(characters), 2))\n",
    "    # if only one character in scene, add lonely node\n",
    "    if len(characters) == 1:\n",
    "        lonely_nodes.append(characters[0])\n",
    "    \n",
    "    return edge_list, lonely_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MAIN EXTRACTOR FUNCTION\n",
    "# take first n acts from drama\n",
    "def edge_list_extractor_extractor(list_of_soup_segments):\n",
    "    \"\"\"\n",
    "    Takes list of soup elements, returns edge list for shared scenes, and if no scenes, just shared acts.\n",
    "    This is GerDracor specific, and should only be used for the 128 dramas --> len(acts) == 5 and len(cast_list) > 5 and genre in ['Tragedy', 'Comedy']\n",
    "    \"\"\"\n",
    "    \n",
    "    lonely_nodes = []\n",
    "    edge_list_in_iteration = []\n",
    "\n",
    "    # iterate over n acts\n",
    "    for c, act in enumerate(list_of_soup_segments, start=1):\n",
    "        if act is None:\n",
    "            if c == 1 or c == len(list_of_soup_segments):\n",
    "                continue\n",
    "            else:\n",
    "                raise ValueError(f'ACT {c} IS NONE IN {name}')\n",
    "                \n",
    "        configurations_in_act = act.find_all('div', {'type': 'configuration'})\n",
    "        locations_in_act = act.find_all('div', {'type': 'location'})\n",
    "        scenes_in_act = act.find_all('div', {'type': 'scene'})\n",
    "        \n",
    "        check_list = [len(configurations_in_act) > 0, len(locations_in_act) > 0, len(scenes_in_act) > 0]\n",
    "        if len([c for c in check_list if c is True]) > 1:\n",
    "            raise ValueError(f'UNACCOUNTED STRUCTURE OF DRAMA - {name} ! {check_list}')\n",
    "        \n",
    "        # IF IT HAS SCENES\n",
    "        if len(scenes_in_act) > 0:\n",
    "            for scene in scenes_in_act:\n",
    "                if scene.find('div', {'type': 'scene'}) is not None:\n",
    "                    for inner_scene in scene.find_all('div', {'type': 'scene'}):\n",
    "                        if inner_scene.find('div', {'type': 'scene'}):\n",
    "                            print(f'SCENE WITHIN SCENE WITHIN SCENE in {name}', file=sys.stderr)\n",
    "                        scene_edge_list, lonely_nodes = one_appearance_unit_edge_list(inner_scene, lonely_nodes)\n",
    "                        edge_list_in_iteration += scene_edge_list\n",
    "                else:\n",
    "                    scene_edge_list, lonely_nodes = one_appearance_unit_edge_list(scene, lonely_nodes)\n",
    "                    edge_list_in_iteration += scene_edge_list\n",
    "\n",
    "        # IT HAS LOCATIONS\n",
    "        elif len(locations_in_act) > 0:\n",
    "            for location in locations_in_act:\n",
    "                    \n",
    "                location_edge_list, lonely_nodes = one_appearance_unit_edge_list(location, lonely_nodes)\n",
    "                edge_list_in_iteration += location_edge_list\n",
    "        \n",
    "        # IT HAS CONFIGURATIONS\n",
    "        elif len(configurations_in_act) > 0:\n",
    "            for configuration in configurations_in_act:\n",
    "                configuration_edge_list, lonely_nodes = one_appearance_unit_edge_list(configuration, lonely_nodes)\n",
    "                edge_list_in_iteration += configuration_edge_list\n",
    "        \n",
    "        # IT HAS NO SCENES AND NO LOCATIONS AND NO CONFIGURATIONS\n",
    "        else:\n",
    "            if act.find('div') is not None:\n",
    "                raise ValueError(f'Unaccounted div type in {name} !')\n",
    "            act_edge_list, lonely_nodes = one_appearance_unit_edge_list(act, lonely_nodes)\n",
    "            edge_list_in_iteration += act_edge_list\n",
    "\n",
    "    return edge_list_in_iteration, lonely_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grillparzer-des-meeres-und-der-liebe-wellen-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing goethe-clavigo-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing grillparzer-medea-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing hauptmann-carl-tobias-buntschuh-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing ludwig-die-makkabaeer-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing toerring-agnes-bernauerin-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing beer-struensee-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing hofmannsthal-der-turm-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing wieland-lady-johanna-gray-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing gottschedin-das-testament-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing laube-gottsched-und-gellert-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing immermann-andreas-hofer-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing gryphius-catharina-von-georgien-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing collin-coriolan-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing goethe-der-grosskophta-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing hebbel-judith-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing grabbe-kaiser-heinrich-der-sechste-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing klinger-das-leidende-weib-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing pfeil-lucie-woodvil-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing cronegk-der-misstrauische-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing hebbel-herodes-und-mariamne-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing kotzebue-der-wirrwarr-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing gryphius-papinianus-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing muellner-koenig-yngurd-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing hebbel-siegfrieds-tod-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing brentano-ponce-de-leon-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing iffland-figaro-in-deutschland-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing holz-sozialaristokraten-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing goethe-die-natuerliche-tochter-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing leisewitz-julius-von-tarent-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing schnitzler-professor-bernhardi-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing eichendorff-der-letzte-held-von-marienburg-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing goethe-faust-der-tragoedie-zweiter-teil-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing schiller-die-verschwoerung-des-fiesco-zu-genua-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing sudermann-der-bettler-von-syrakus-1-2-3-4-5-6\n",
      "---------------\n",
      "Processing hartleben-rosenmontag-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing bleibtreu-weltgericht-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing klingemann-faust-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing quistorp-der-hypochondrist-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing panizza-das-liebeskonzil-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing voss-faust-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing koerner-zriny-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing hebbel-gyges-und-sein-ring-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing gryphius-leo-armenius-oder-fuersten-mord-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing seume-miltiades-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing braun-von-braunthal-faust-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing platen-der-romantische-oedipus-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing hebbel-der-diamant-1-2-3-4-5-6\n",
      "---------------\n",
      "Processing laube-struensee-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing kotzebue-die-spanier-in-peru-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing grillparzer-ein-bruderzwist-in-habsburg-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing sturz-julie-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing grabbe-herzog-theodor-von-gothland-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing lassalle-franz-von-sickingen-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing brawe-brutus-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing lenz-der-neue-menoza-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing gottsched-der-sterbende-cato-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing hofmannsthal-der-unbestechliche-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing grillparzer-koenig-ottokars-glueck-und-ende-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing braun-mutter-maria-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing lessing-minna-von-barnhelm-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing dehmel-michel-michael-1-2-3-4-5-6\n",
      "---------------\n",
      "Processing krueger-die-candidaten-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing schiller-maria-stuart-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing lenz-die-freunde-machen-den-philosophen-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing schiller-kabale-und-liebe-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing gryphius-horribilicribrifax-teutsch-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing heiseler-die-kinder-godunofs-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing hebbel-genoveva-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing neuber-das-schaeferfest-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing grillparzer-ein-treuer-diener-seines-herrn-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing schiller-die-jungfrau-von-orleans-1-2-3-4-5-6\n",
      "---------------\n",
      "Processing gryphius-carolus-stuardus-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing ludwig-der-erbfoerster-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing gottschedin-die-pietisterey-im-fischbein-rocke-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing lessing-emilia-galotti-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing gryphius-cardenio-und-celinde-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing scheerbart-es-lebe-europa-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing holz-sonnenfinsternis-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing hebbel-agnes-bernauer-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing lenz-der-hofmeister-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing gutzkow-richard-savage-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing kraus-die-letzten-tage-der-menschheit-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing hebbel-kriemhilds-rache-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing kotzebue-pagenstreiche-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing grabbe-kaiser-friedrich-barbarossa-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing cronegk-codrus-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing grillparzer-die-ahnfrau-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing immermann-das-gericht-von-st-petersburg-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing panizza-nero-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing wildgans-armut-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing lessing-der-freigeist-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing essig-ueberteufel-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing wilbrandt-gracchus-der-volkstribun-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing goethe-egmont-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing hauptmann-florian-geyer-1-2-3-4-5-6\n",
      "---------------\n",
      "Processing heyse-don-juans-ende-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing grillparzer-die-juedin-von-toledo-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing brachvogel-narziss-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing klinger-die-zwillinge-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing gutzkow-zopf-und-schwert-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing scheerbart-okurirasuna-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing grillparzer-libussa-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing schlegel-der-triumph-der-guten-frauen-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing essig-der-frauenmut-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing grillparzer-sappho-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing schiller-wallensteins-tod-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing weissenthurn-das-manuscript-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing brawe-der-freigeist-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing lessing-miss-sara-sampson-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing tieck-karl-von-berneck-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing gutzkow-das-urbild-des-tartueffe-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing gutzkow-uriel-acosta-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing schlegel-der-geschaeftige-muessiggaenger-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing heiseler-peter-und-alexej-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing weissenthurn-welche-ist-die-braut-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing holz-ignorabimus-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing grillparzer-weh-dem-der-luegt-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing wildgans-dies-irae-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing laube-monaldeschi-1-2-3-4-5-6\n",
      "---------------\n",
      "Processing engel-eid-und-pflicht-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing schlegel-canut-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing anonym-der-deutsche-student-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing platen-die-verhaengnisvolle-gabel-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing uhland-ernst-herzog-von-schwaben-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing bleibtreu-ein-faust-der-that-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing scheerbart-herr-kammerdiener-kneetschke-1-2-3-4-5-6-7\n",
      "---------------\n",
      "Processing kleist-die-familie-schroffenstein-1-2-3-4-5-6-7\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# MAIN CUMULATIVE NETWORKS GENERATION FOR GERDRACOR\n",
    "\n",
    "cumulative_G_list_ger = defaultdict(dict)\n",
    "\n",
    "for name, soup in ger_soups.items():\n",
    "    \n",
    "    # Metadata annotation for dict\n",
    "    cumulative_G_list_ger[name]['soup'] = soup\n",
    "    genre_tag = soup.find('textClass').find('keywords').find('term', {'type': 'genreTitle'})\n",
    "    cumulative_G_list_ger[name]['genre'] = genre_tag.get_text(strip=True)\n",
    "    cumulative_G_list_ger[name]['title_pretty'] = soup.find('title').get_text(strip=True)\n",
    "    \n",
    "    # These are used to determine what structural elements there are.\n",
    "    all_acts = soup.find_all('div', {'type': 'act'})\n",
    "    prologue = soup.find('div', {'type': 'prologue'})\n",
    "    epilogue = soup.find('div', {'type': 'epilogue'})\n",
    "    \n",
    "    # Whole calculation to \n",
    "    whole_list = []\n",
    "    whole_list.extend(all_acts)\n",
    "    \n",
    "    if prologue is not None:\n",
    "        whole_list.insert(0, prologue)\n",
    "    elif prologue is None:\n",
    "        whole_list.insert(0, None)\n",
    "    \n",
    "    if epilogue is not None:\n",
    "        whole_list.append(epilogue)\n",
    "    elif prologue is None:\n",
    "        whole_list.append(None)\n",
    "        \n",
    "    edge_list, lonely_nodes = edge_list_extractor_extractor(whole_list)\n",
    "    # create network from edge list\n",
    "    whole_drama = nx.from_edgelist(set(edge_list))\n",
    "\n",
    "    # add those with independent scenes\n",
    "    for lonely_node in lonely_nodes:\n",
    "        if lonely_node not in whole_drama.nodes:\n",
    "            whole_drama.add_node(lonely_node)\n",
    "    \n",
    "    cumulative_G_list_ger[name]['whole'] = whole_drama\n",
    "    \n",
    "    # Culminative calculation\n",
    "    print(f'Processing {name}', end='')\n",
    "    # create iterations for 1, 1-2, 1-2-3, ... acts\n",
    "    for iteration_round in range(1, len(whole_list)+1):  # was all_acts\n",
    "        \n",
    "        # take first n acts from drama\n",
    "        acts_included = whole_list[:iteration_round]  # was all_acts\n",
    "        print(f'-{len(acts_included)}', end='')\n",
    "        \n",
    "        edge_list_in_iteration, lonely_nodes = edge_list_extractor_extractor(acts_included)\n",
    "\n",
    "        # create network from edge list\n",
    "        n_acts_whole = nx.from_edgelist(set(edge_list_in_iteration))\n",
    "\n",
    "        # add those with independent scenes\n",
    "        for lonely_node in lonely_nodes:\n",
    "            if lonely_node not in n_acts_whole.nodes:\n",
    "                n_acts_whole.add_node(lonely_node)\n",
    "        \n",
    "        if iteration_round == 1:\n",
    "            label = 'epilogue'\n",
    "            if whole_list[0] is None:\n",
    "                n_acts_whole = None\n",
    "        elif iteration_round == len(whole_list):\n",
    "            label = 'prologue'\n",
    "            if whole_list[-1] is None:\n",
    "                n_acts_whole = None\n",
    "        else:\n",
    "            label = f\"acts_{'-'.join([str(i) for i in range(1, iteration_round)])}\"\n",
    "            \n",
    "        cumulative_G_list_ger[name][label] = n_acts_whole\n",
    "    print('\\n---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GERDRACOR STRUCTURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  act\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "  act\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "  act\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "  act\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "  act\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "  epilogue\n",
      "    Dramatis_Personae\n",
      "    set\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "  act\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "  act\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "  act\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "  act\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "  act\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "    scene\n",
      "  epilogue\n"
     ]
    }
   ],
   "source": [
    "def div_structure(unit, indent):\n",
    "    for div in unit.find_all('div', recursive=False):\n",
    "        print(f\"\"\"{indent*\" \"}{div['type']}\"\"\")\n",
    "        div_structure(div, indent+2)\n",
    "for name, soup in ger_soups.items():\n",
    "    if name in ['hebbel-genoveva', 'neuber-das-schaeferfest']:\n",
    "        body = soup.find('body')\n",
    "        div_structure(body, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GERDRACOR CSV WRITING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = 'gerdracor_cumulative1.csv'\n",
    "column_names = ['title', 'title_pretty', 'genre']\n",
    "metric_names = ['density', 'diameter', 'average_clustering']\n",
    "most_acts = max([len(v) for v in cumulative_G_list_ger.values()]) - 1\n",
    "\n",
    "for act in range(1, 5+1):\n",
    "    for n in metric_names:\n",
    "        column_names.append(f\"acts_{'-'.join([str(n) for n in range(1, act+1)])}_{n}\")\n",
    "        \n",
    "for section in ['epilogue', 'prologue']:\n",
    "    for n in metric_names:\n",
    "        column_names.append(f\"{section}_{n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_result_dict = defaultdict(dict)\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=column_names)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for name, drama_data in cumulative_G_list_ger.items():\n",
    "        drama_dict = {'title': name,\n",
    "                      'title_pretty': drama_data['title_pretty'],\n",
    "                      'genre': drama_data['genre']\n",
    "             }\n",
    "        \n",
    "        for section in ['epilogue', 'prologue']:\n",
    "            if drama_data[section] is not None:\n",
    "                density = nx.density(drama_data[section])\n",
    "                diameter = nx.diameter(get_largest_G(drama_data[section], name))\n",
    "                average_clustering = nx.average_clustering(drama_data[section])\n",
    "            else:\n",
    "                density, diameter, average_clustering = None, None, None\n",
    "                \n",
    "            drama_dict[f\"{section}_density\"] = density\n",
    "            drama_dict[f'{section}_diameter'] = diameter\n",
    "            drama_dict[f'epilogue_average_clustering'] = average_clustering\n",
    "        \n",
    "        for act in range(1, 5+1):\n",
    "            acts_name = f\"acts_{'-'.join([str(n) for n in range(1, act+1)])}\"\n",
    "            if acts_name in drama_data.keys():\n",
    "                drama_dict[f\"{acts_name}_density\"] = nx.density(drama_data[acts_name])\n",
    "                drama_dict[f'{acts_name}_diameter'] = nx.diameter(get_largest_G(drama_data[acts_name], name))\n",
    "                drama_dict[f'{acts_name}_average_clustering'] = nx.average_clustering(drama_data[acts_name])\n",
    "        ger_result_dict[name] = drama_dict\n",
    "        writer.writerow(drama_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CROSSCHECK GERDRACOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_gerdracor_metadata_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [187]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metadata_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43moriginal_gerdracor_metadata_file\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'original_gerdracor_metadata_file' is not defined"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(original_gerdracor_metadata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata_df = metadata_df.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_matching = set()\n",
    "df_data = []\n",
    "for name, drama_dict in cumulative_G_list_ger.items():\n",
    "    row = {'name': name}\n",
    "    \n",
    "    G = drama_dict['whole']\n",
    "    my_density = nx.density(G)\n",
    "    my_diameter = nx.diameter(get_largest_G(G, name))\n",
    "    my_average_clustering = nx.average_clustering(G)\n",
    "    \n",
    "    if round(my_density, 2) != round(metadata_df.loc[name]['density'], 2):\n",
    "        row['density'] = metadata_df.loc[name]['density']\n",
    "        row['my_density'] = my_density\n",
    "    if round(my_diameter, 2) != round(metadata_df.loc[name]['diameter'], 2):\n",
    "        row['diameter'] = metadata_df.loc[name]['diameter']\n",
    "        row['my_diameter'] = my_diameter\n",
    "    if round(my_average_clustering, 2) != round(metadata_df.loc[name]['averageClustering'], 2):\n",
    "        row['avg_clu'] = metadata_df.loc[name]['averageClustering']\n",
    "        row['my_avg_clu'] = my_average_clustering\n",
    "    if len(row) > 1:\n",
    "        df_data.append(row)\n",
    "pd.DataFrame(df_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GERDRACOR STATS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_dracor_stats = []\n",
    "for name, soup in ger_soups.items():\n",
    "    \n",
    "    acts = len(soup.find_all('div', {'type': 'act'})) > 0\n",
    "    prologue = len(soup.find_all('div', {'type': 'prologue'})) > 0\n",
    "    scenes = len(soup.find_all('div', {'type': 'scene'})) > 0\n",
    "    configurations = len(soup.find_all('div', {'type': 'configuration'})) > 0\n",
    "    locations = len(soup.find_all('div', {'type': 'location'})) > 0\n",
    "    \n",
    "    if all(elem is False for elem in [acts, scenes, configurations, locations]):\n",
    "        raise ValueError(f'CONTENT HOLDER DIVS NOT ACCOUNTED FOR {name}')\n",
    "\n",
    "    # has acts and scenes\n",
    "    if acts and scenes:\n",
    "        if scenes is False:\n",
    "            raise ValueError(f'{name}')\n",
    "        scene_lens_in_acts = []\n",
    "        for act in soup.find_all('div', {'type': 'act'}):\n",
    "            scenes_list = act.find_all('div', {'type': 'scene'})\n",
    "            scene_lens_in_acts.append(len(scenes_list))\n",
    "        if all(elem > 0 for elem in scene_lens_in_acts):\n",
    "            all_acts_have_scenes = True\n",
    "            \n",
    "        elif any(elem > 0 for elem in scene_lens_in_acts):\n",
    "            all_acts_have_scenes = False\n",
    "    else:\n",
    "        all_acts_have_scenes = False\n",
    "        \n",
    "    ger_dracor_stats.append({'name': name, \n",
    "                             'prologue': prologue,\n",
    "                             'acts': acts, \n",
    "                             'scenes': scenes, \n",
    "                             'all_acts_have_scenes': all_acts_have_scenes,\n",
    "                             'configurations': configurations, \n",
    "                             'locations': locations})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gerdracor_content_stats.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['name', 'prologue', 'acts', 'scenes', 'all_acts_have_scenes', 'configurations', 'locations'])\n",
    "    writer.writeheader()\n",
    "    for d in ger_dracor_stats:\n",
    "        writer.writerow(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
